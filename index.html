<!DOCTYPE HTML>
<!--
	Dimension by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<!-- 	This is the "Head" of the HTML document.
		It contains information that isn't displayed on the actual page, but is useful
		for the web browser when loading the page	
	-->
	<head>
		<!-- This is the title of the page. It is the text that appears inside this pages tab in your web browser -->
		<title>Christian Segnou Web CV</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<!-- 	This links to this page's CSS, which is contained in the folder assets/css and the file main.css.
			 If you want to edit the styling of this page, you should edit the file assets/css/main.css
			 If you have a new CSS file you'd like to add with custom styling, you should link to it here using:
			 <link rel="stylesheet" href="assets/css/my-new-css-file.css"/> 
		-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!-- 	In the case that the user's browser does not support JavaScript (unlikely, but possible), the page
			will load a separate set of CSS stylings from the file assets/css/noscript.css
			Any HTML contained inside <noscript></noscript> tags will be loaded in the event that JavaScript is not
			available. 
		-->
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		
	</head>
	<!-- The body is the location where your site's content will go -->
	<body class="is-preload">

			<!-- This "div" wraps around all of our content and just changes how things are layed out -->
			<div id="wrapper">

					<!-- This is where the content that appears on page load exists -->
					<header id="header">
						<!-- This is the main content of the front page -->
						<div class="content">
							<div class="inner">
								<!-- Here is a heading where you can put your name -->
								<h2>
									Christian Marius 
								</h2>
								
								<h1>
									 NGUIEPE SEGNOU
								</h1>
								<!-- 	Here is an image where you can put a picture of you. 
									You can change the width and height attributes below to change how large
									your image is.

									Try adding "border-radius: 50%;" to the style attribute.
								-->
								<img src="images/photo_ret_site.jpg" style="width:369px; height:406px; object-fit: cover;">
								<!-- 	Here is a paragraph where you can put your position and institution, or
									a short line about yourself.
								-->
								<p>
									Machine Learning Engineer | Data Engineer | Physicist | Researcher
								</p>
								<!-- Here is a paragraph where you can put a link to your CV -->
								<p>
									<!-- Note you will want to change where this points to! -->
									<a href="images/Christian_Segnou_CV_Bon.pdf" target="_blank">Curriculum Vitae</a>
								</p>
							</div>
						</div>
						<!-- This is the navigation menu -->
						<nav>
							<!-- This element makes an "Unordered List" -->
							<ul>
								<!-- This is a "List Item" -->
								<li>
									<!-- 	Note that this links to #about, which will move the page to wherever
										the element with the id "about" exists.
									-->
									<a href="#about">About Me</a>
								</li>
								<li>
									<a href="#expertise">Expertise</a>
								</li>
								<li>
									<a href="#projects">Projects</a>
								</li>
								<li>
									<a href="#contact">Contact</a>
								</li>
								<!-- 	You can add another button to your navigation menu by adding
									another List Item with a link inside of it.
								-->
								<!-- <li>
									<a href="images/Astronomy.jpg">My CV</a>
								</li> -->
							</ul>
						</nav>
					</header>

					<!-- These are the "cards" that appear when you click the buttons on the main page -->
					<div id="main">
						<!-- 	Here is the "About Me" card. Note it has the id "about", which the previous 
							button links to.
						-->
						<article id="about">
							
							<!-- Here is an image that you can customize for this page -->
							<span class="image main">
								<img src="images/aboutme.jpeg" alt=""/>
							</span>
							
							<h2 class="major">About Me</h2>
							
							<!-- Here are three paragraphs where you can fill out information about yourself -->
							<p>
								<br>
								
								I recently completed my Ph.D. in Physics (I am waiting for defense). 
								I am excited to transition my skills and experience towards a career in data science, 
								where I can apply my strong foundation in mathematical modeling, data analysis, 
								and programming to solve complex problems and gain new insights from large datasets.
									
							</p>

							<br>

							<p>

								<h3>WORK EXPERIENCE :</h3>
								
								<br>
        
         						‚óè	<b>PYTHON TEACHER -  <a href="https://www.universite-paris-saclay.fr/"> Paris-Saclay University üë®üèæ‚Äçüè´ </a> - 91400 Osrsay - from 2019 to 2021 </b>
        						<br>
								<br>
								As an experienced Python instructor, I was responsible for developing practical work, and courses for students ranging from L1 to L3 levels. 
								The program I developed covered a broad range of topics, from the fundamentals of Python programming to the introduction concepts of machine learning and deep learning.
								<br>
								<br>
								Throughout my role as an instructor, I focused on creating a learning environment that was both engaging and effective. I used a variety of teaching methods, 
								including hands-on exercises, group discussions, and real-world examples to help students understand complex concepts and gain practical skills.
								In addition to providing personalized feedback to individual students and,  I also  facilitated collaborative learning opportunities. 
								<br>
								<br>
								By fostering a supportive and inclusive learning environment, I was able to ensure that all students were able to fully engage with the course content and achieve their full potential.
								Overall, my experience as a Python instructor has equipped me with a deep understanding of the principles of effective teaching, 
								as well as extensive knowledge of the Python programming language and its applications in machine learning and deep learning.
        
								

								<br>
        						<br>
								<br>
        						‚óè	<b>PHD CANDIDATE - <a href="https://www.c2n.universite-paris-saclay.fr/en/"> C2N üë®üèæ‚Äçüî¨ </a> - 91120 Palaiseau - from 08/2018 to - </b>
        						<br>
								<br>
        						During my Ph.D. studies in Physics in CENTRE FOR NANOSCIENCE AND NANOTECHNOLOGY (C2N) lab, I focused on researching the dynamics of magnetic domain walls in magnetic thin films.
								To conduct this research, I utilized the Kerr magneto-optical microscopy technique to visualize the behavior of these magnetic structures. 
								<br>
								<br>
								In the experimental phase of my research, I was responsible for taking precise measurements and analyzing the resulting data using various programming libraries such as Numpy, Matplotlib, Scikit-learn, and Pandas. Additionally, 
								I utilized these libraries to perform statistical analysis and data visualization tasks that helped to support my findings.
								<br>
								<br>
								In the modeling phase of my research, I developed complex mathematical models to simulate the behavior of these magnetic structures using the Scipy and Sympy libraries in addition to those mentioned above.
								 By utilizing these tools, I was able to gain a deeper understanding of the underlying physics of magnetic domain walls, which led to significant advancements in our knowledge of their behavior.
								 <br>
								 <br>
								Overall, my research on magnetic domain walls required a strong proficiency in both experimental techniques and programming, and I was able to successfully integrate these skills to contribute to the field of Physics. 
        
        						<br>
        						<br>
								<br> 
								‚óè	<b>TOTAL MARKETING FRANCE - <a href="https://services.totalenergies.fr/"> TOTAL üöÇ üõ¢ </a> - 93742 Nanterre - 08/2018 </b>
								<br>
								<br>
        						As part of my responsibilities, I was tasked with overseeing the routine evaluations of the wagon fleet as they transported products from the refineries to the distribution centers. 
								This involved closely monitoring the regular inspections of each wagon and ensuring that any necessary maintenance or repairs were promptly carried out. 
								By effectively managing these periodic revisions, I helped to maintain the high standards of safety, efficiency, and reliability required for the successful transportation of goods.  
	   							<br>
    							<br>
								<br>
								


							</p>



							<p>
								<!-- Items contained in <b> tags are bolded! -->
								<h3>EDUCATION :</h3>
						
								‚óè	<b>BOOTCAMP DATA SCIENCE FULLSTACK - <a href="https://services.totalenergies.fr/"> JEDHA </a> - 75003 Paris - from 01/2023 to 05/2023 </b>
								<br>
								<br>
								‚óè	<b>PHD CANDIDATE IN NANOELECTRONICS/SPINTRONICS - <a href="https://www.universite-paris-saclay.fr/"> Paris-Saclay University üë®üèæ‚Äçüî¨ </a> - 91120 Palaiseau - from 10/2018 to -</b>
								<br>
								<br>
								‚óè	<b>MASTER'S DEGREE IN NANOPHYSICS - <a href="https://www.universite-paris-saclay.fr/"> Paris-Saclay University üë®üèæ‚Äçüéì </a> - 91120 Palaiseau - from 09/2017 to - 06/2018</b>
								<br>
								<br>
								‚óè	<b>MASTER'S DEGREE IN MATERIAL SCIENCES - <a href="https://uy1.uninet.cm"> YAOUNDE I University üë®üèæ‚Äçüéì </a> - YAOUNDE - from 09/2015 to - 06/2017</b>
						
							
							</p>
							
						</article>

						<!-- Here is the "Research" card. -->
						<article id="expertise">
							
							<span class="image main">
								<img src="images/expertise.jpeg" alt=""/>
							</span>
							<h2 class="major">Expertise</h2>

							<p>
								<!-- 	A <br> tag puts in a break so that the text that follows it displays on
									the next line.
								-->

							
								<h3>DataScience and Machine Learning</h3>
								<br>
								Throughout my training in the Jedha Bootcamp program, I had the opportunity to develop a diverse range of skills through the completion of various projects. 
								These projects allowed me to gain practical experience in applying data science concepts and techniques to real-world scenarios, 
								and helped me to further hone my abilities in data preprocessing, machine learning, data visualization, and project management. 
								Overall, the hands-on nature of the Jedha Bootcamp program provided me with invaluable experience and knowledge that I am eager to apply to future endeavors in the field of data science.
								<br>
								<br>
								
								
								‚óè <b>Programming Languages and Version Control</b> :
								<br> 
								With <b>Python</b>, I can efficiently develop and implement various software solutions, leveraging its extensive libraries and frameworks. <b>Linux</b> is a familiar environment for me, 
								enabling me to navigate and utilize its command-line interface effectively. Additionally, my expertise extends to using <b>Git</b> and <b>GitHub</b> for version control, 
								allowing me to collaborate seamlessly with other developers and manage code repositories efficiently. These skills enable me to contribute to projects effectively, ensuring smooth development and streamlined workflows.
								

								<br>
								<br>
								‚óè <b>Data Manipulation</b> :
								<br>
								I possess a strong proficiency in data manipulation, employing powerful tools such as <b>NumPy</b>, <b>Pandas</b>, and <b>SQL</b>. With <b>NumPy</b>, I can efficiently perform numerical computations and handle large arrays of data, 
								enabling complex mathematical operations and statistical analyses. <b>Pandas</b>, on the other hand, empowers me to manipulate and analyze structured data effortlessly, providing extensive functionalities for data cleaning, 
								transformation, and exploration. Additionally, my expertise in <b>SQL</b> allows me to query and extract data from relational databases, ensuring efficient data retrieval and manipulation. With these skills, 
								I am well-equipped to handle diverse datasets, extract valuable insights, and drive informed decision-making processes.

								<br>
								<br>



								‚óè <b>Data Visualization</b> :
								<br>
								I possess skills in data visualization, utilizing popular libraries such as <b>Matplotlib</b>, <b>Seaborn</b>, and <b>Plotly</b>. With <b>Matplotlib</b>, I can create a wide range of static visualizations, including line plots, bar charts, histograms, 
								and scatter plots, to effectively communicate insights from data. <b>Seaborn</b> enhances my capabilities by providing a high-level interface for producing attractive and informative statistical graphics. Furthermore, 
								<b>Plotly</b> empowers me to build interactive and dynamic visualizations, enabling users to explore and interact with data directly. With these skills, I can transform complex datasets into visually appealing and intuitive representations, 
								enabling clear communication and facilitating data-driven decision-making.
								<br>
								<br>


								‚óè <b>Data Collection and Management</b> :
								<br>
								I possess a skill set in data collection and management, encompassing various tools and techniques. My expertise in web scraping allows me to efficiently extract data from websites using frameworks like <b>Scrapy</b>, 
								enabling the automated collection of structured information. Additionally, I am adept at managing data on cloud platforms such as <b>Amazon Web Services (AWS)</b>, specifically using services like <b>Amazon IAM (Identity and Access Management)</b>, 
								<b>Amazon S3 (Simple Storage Service)</b>, and <b>Amazon RDS (Relational Database Service)</b>. These skills enable me to securely store and manage data in the cloud, ensuring scalability, reliability, and ease of access. 
								Furthermore, my proficiency in SQLAlchemy allows me to interact with databases, providing a powerful and flexible toolkit for data manipulation, querying, and management. 
								With these skills, I can effectively gather, organize, and store data, enabling seamless data workflows and facilitating analysis and insights extraction.
								<br>
								<br>


								‚óè <b>Data Collection and Management</b> :
								<br>
								I possess skills in big data processing and distributed computing, leveraging powerful technologies such as <b>Apache Spark</b>, <b>Spark SQL</b>, <b>PySpark RDD</b>, and <b>Databricks</b>. With <b>Apache Spark</b>, 
								I can efficiently process and analyze large-scale datasets in a distributed and parallelized manner, enabling high-performance data processing. <b>Spark SQL</b> enhances my capabilities by providing a SQL interface for querying structured data, enabling seamless integration with existing SQL-based workflows. 
								Moreover, my expertise in <b>PySpark RDD (Resilient Distributed Datasets)</b> allows me to manipulate and transform data in a distributed computing environment using Python, unlocking the full potential of Spark's capabilities.
								Additionally, I have experience working with Databricks, a collaborative and cloud-based platform for big data analytics, providing an environment for interactive data exploration, visualization, and collaboration. 
								With these skills, I can effectively handle big data challenges, process vast amounts of information, and derive meaningful insights from complex datasets.
								<br>
								<br>


								‚óè <b>Machine Learning, Deep Learning and Modeling</b> :
								<br>
								I possess comprehensive skills in machine learning, deep learning, and modeling, employing a range of powerful tools and techniques. With <b>Scikit-learn</b>, I can apply various algorithms, including <b>linear and logistic regression</b>, 
								<b>decision trees</b>, <b>random forests</b>, <b>support vector machines (SVM)</b>, <b>ensemble learning</b>, <b>K-means clustering</b>, <b>DBSCAN</b>, and dimensionality reduction techniques like <b>principal component analysis (PCA)</b>. 
								Furthermore, I have expertise in <b>natural language processing (NLP)</b> and <b>latent semantic analysis (LSA)</b>, enabling me to analyze and extract meaningful information from textual data.
								In addition, my proficiency extends to <b>TensorFlow and Keras</b>, two widely used frameworks for deep learning. I am experienced in building and training neural networks, <b>convolutional neural networks (CNNs)</b>, 
								<b>transfer learning</b>, <b>fine-tuning</b> models, <b>generative adversarial networks (GANs)</b>, <b>word embeddings</b>, <b>recurrent neural networks (RNNs)</b>, and <b>encoder-decoder architectures with attention mechanisms</b>. 
								These skills allow me to develop and deploy cutting-edge deep learning models for tasks like image classification, natural language processing, and sequence generation.
								With this broad skill set, I am well-equipped to tackle various machine learning and deep learning problems, from data preprocessing and feature engineering to model selection, training, and evaluation. 
								I can create robust and accurate models that drive insights and make predictions across diverse domains.
								<br>
								<br>



								‚óè <b>Deployment and Distributed Machine Learning</b> :
								<br>
								I possess extensive skills in deployment and distributed machine learning, utilizing a range of powerful technologies and platforms. With <b>Docker</b>, I can containerize applications and create reproducible environments, ensuring seamless deployment across different systems. 
								<b>Kubernetes</b> further enhances my capabilities by enabling efficient management and orchestration of containers in distributed environments, ensuring scalability and fault tolerance.
								Additionally, I am proficient in MLflow, a platform for managing the machine learning lifecycle, allowing me to track experiments, package and deploy models, and monitor performance. 
								Ray is another tool I am familiar with, which facilitates distributed computing and parallel processing, enabling efficient scaling and optimization of machine learning tasks.
								Furthermore, I have expertise in utilizing <b>Helm</b>, a package manager for <b>Kubernetes</b>, to simplify the deployment and management of complex applications. 
								I also possess in-depth knowledge of various AWS services such as <b>IAM (Identity and Access Management)</b>, <b>S3 (Simple Storage Service)</b>, <b>RDS (Relational Database Service)</b>, <b>Redshift (data warehousing)</b>, <b>EC2 (Elastic Compute Cloud)</b>, and <b>EKS (Elastic Kubernetes Service)</b>. 
								Similarly, I am skilled in working with <b>GCP services</b> such as <b>IAM (Identity and Access Management)</b> and <b>Kubernetes Engine</b>, allowing me to deploy and manage machine learning applications on the <b>Google Cloud Platform</b>.
								Moreover, I have experience with <b>Heroku</b>, a cloud platform for application deployment, and <b>FastAPI</b>, a Python web framework for building efficient and scalable APIs. These skills enable me to deploy machine learning models and expose them as APIs, facilitating easy integration with other systems and applications.
								With these skills, I can effectively deploy and manage distributed machine learning systems, leveraging containerization, orchestration, and cloud platforms to ensure scalable and reliable deployment of models in production environments.
								<br>
								<br>


								‚óè <b>Data Pipelines and Automation & Workflow</b> :
								<br>
								I possess comprehensive skills in data pipelines, automation, and workflow management, utilizing a range of powerful tools and technologies. With <b>Apache Airflow</b>, I can design, schedule, and monitor complex data pipelines, ensuring efficient and reliable data movement and transformation. 
								This allows for the automation of tasks and the orchestration of workflows, streamlining data processing and ensuring data integrity.
								I am also experienced in <b>Apache Kafka</b>, a distributed streaming platform, which enables real-time data streaming and processing. This allows for the seamless integration of various data sources and the creation of robust and scalable data pipelines.
								Furthermore, I have expertise in <b>Airbyte</b>, an open-source data integration platform, which facilitates the extraction, transformation, and loading (<b>ETL</b>) of data from multiple sources into a centralized location. This enables efficient data synchronization and integration across different systems.
								Additionally, my skills extend to <b>Neo4j</b>, a graph database, which enables the efficient storage, querying, and analysis of interconnected data. This is particularly useful for managing complex relationships and conducting graph-based data analytics.
								Furthermore, I have experience with <b>Zapier</b>, an automation tool that allows for the creation of workflows and the seamless integration of various applications and services. This enables the automation of repetitive tasks and the efficient flow of data between different systems.
								With these skills, I can design and implement robust data pipelines, automate data workflows, and ensure the smooth and efficient movement of data across different systems and applications.
								<br>
								<br>



								‚óè <b>Machine Learning Monitoring</b> :
								<br>
								I possess strong skills in Machine Learning monitoring, specifically in the area of <b>data drift</b>, using the tool <b>Evidently</b>. <b>Evidently</b> is a powerful platform that enables the monitoring and detection of <b>data drift</b> in machine learning models. With this tool,
								I can track changes in the statistical properties of incoming data and identify when the underlying data distribution has shifted significantly. This allows for the proactive identification of potential performance degradation or bias in Machine Learning models due to changes in data over time. 
								By utilizing <b>Evidently</b>, I can ensure the ongoing accuracy and reliability of deployed Machine Learning models by monitoring and addressing data drift effectively.
								<br>
								<br>

							
							</p>
							<br>
							<p>
								<h3>Research</h3>
								<br>
								As a Ph.D. graduate in Physics with expertise in magnetic domain wall dynamics, I have a strong foundation in both experimental techniques and theoretical modeling. 
								My research focused on understanding the behavior of magnetic domain walls in magnetic thin films, and I utilized various experimental methods such as Kerr magneto-optical microscopy to visualize their dynamic behavior.
								<br>
								<br>
								In addition to experimental techniques, I have extensive knowledge of data analysis and programming, particularly in using Python and its scientific computing libraries such as Numpy, Matplotlib, and Pandas. 
								I have utilized these tools to perform statistical analysis and data visualization to gain insights from the large datasets generated during my experimental work.
								<br>
								<br>
								My expertise in magnetic domain wall dynamics also extends to the theoretical modeling of their behavior. 
								I have developed complex mathematical models using programming libraries such as Scipy and Sympy to simulate the dynamic behavior of magnetic domain walls. 
								These models have allowed me to gain a deeper understanding of the underlying physics of magnetic domain walls and their interaction with magnetic fields.
								<br>
								<br>
								Moreover, my experience in magnetic domain wall dynamics has also given me a broad understanding of the broader field of condensed matter physics, 
								including the study of magnetic materials and their applications in magnetic memory devices.
								<br>
								<br>
								Overall, my expertise in magnetic domain wall dynamics has equipped me with a strong foundation in experimental techniques, programming, and theoretical modeling, 
								which I believe will enable me to make valuable contributions to a wide range of scientific and engineering applications.



							</p>
						</article>

						<!-- Here is the "Community" card. -->
						<article id="projects">
							
							<span class="image main">
								<img src="images/ia_project.jpeg" alt=""/>
							</span>

							<h1 class="major">projects</h1>


							
							<h1>Table of Contents</h1>
							<ul>
								<li><a href="#project1"><b>Project 1 </b>: Exploratory Data Analysis Speed Dating Project</a></li>
								<li><a href="#project2"><b>Project 2 </b>: Plan your trip with Kayak </a></li>
								<li><a href="#project3"><b>Project 3 </b>: Walmart: Predict Weekly Sales </a></li>
								<li><a href="#project4"><b>Project 4 </b>: Conversion rate challenge </a></li>
								<li><a href="#project5"><b>Project 5 </b>: The North Face e-commerce : boosting online sales ! </a></li>
								<li><a href="#project6"><b>Project 6 </b>: Hot-Zone Recommendation for Uber Drivers </a></li>
								<li><a href="#project7"><b>Project 7 </b>: Spam detector  </a></li>
								<li><a href="#project8"><b>Project 8 </b>: Getaround, Car Rental Check-in and Checkout Analysis </a></li>
								<li><a href="#group_project1"><b>Group Project 1 </b>: Netflix Recommendation Engine </a></li>
								<li><a href="#group_project2"><b>Group Project 2 </b>: Low Emission Zone Control  </a></li>
							</ul>



							<p>
								<h3 id="project1">Project 1 : Exploratory Data Analysis Speed Dating Project</h3>

                                ‚óè <b>Github repository link    :</b>    <a href="https://github.com/ChrisDMI/Speed-dating-project/">   Github speed dating project </a> 
                                <br>
                                <br>

                                ‚óè <b>Overview :</b><br>
                                During a recent speed dating project, I had the opportunity to explore and analyze a dataset of dating preferences from over 500 participants. 
                                The goal of the project was to use the speed dating dataset of Tinder to understand what makes people interested into each other to go on a second date together. 
                                <br>
                                <br>
                                ‚óè <b>Methodology :</b><br>
                                To analyze the data, I used Python and its data analysis libraries, including Pandas, Matplotlib, and Seaborn. 
                                I cleaned and prepared the data by removing duplicate entries, missing values, and outliers. 
                                Then, I used exploratory data analysis techniques to identify patterns and correlations in the data.
                                <br>
                                <br>
                                ‚óè <b>Key Findings :</b><br> 
                                Through my analysis, I identified several key findings, including :<br>

                                üîë Participants who rated their partners higher in attractiveness and shared interest were more likely to go on a second date.
                                <br>
                                üîë Race, religion or income were not significant predictors of whether or not someone would go on a second date.
                                <br>
                                üîë It is very difficult for people to accurately predict their own perceived value in the dating market.
                                <br>
                                <br>
                                These findings suggest that while there are some gender differences in dating preferences, 
                                there are also some universal factors that are important to both men and women.
                                <br>
                                <br>
                                ‚óè <b>Skills Developed:</b><br>
                                Through this project, I developed several skills related to data analysis and communication, including :
                                <br>
                                üõ† <b>Python</b> programming, including <b>Pandas</b>, <b>Matplotlib</b>, <b>Plotly</b> and <b>Seaborn</b>.
                                <br>
                                üõ† Data manipulation and cleaning.
                                <br>
                                üõ† Exploratory data analysis techniques, including data visualization and statistical analysis.
                                <br>
                                üõ† Effective communication of insights and findings to a non-technical audience.


                                <br>
                                <br>
                                ‚óè <b>Context :</b>
                                This speed dating project is just one example of my experience in data analysis and communication that I acquired during my <b>JEDHA BOOTCAMP</b> training.


                                <br>
                                <br>


                            </p>




							<p>
								<h3 id="project2">Project 2 : Plan your trip with Kayak </h3>

                                ‚óè <b>Github repository link    :</b>    <a href="https://github.com/ChrisDMI/Plan-your-trip-with-Kayak.git">   Github Plan your trip with Kayak </a> 
                                <br>
                                <br>

                                ‚óè <b>Overview :</b><br>
                                The Kayak Marketing Team has initiated a new project to address the needs of their users who are planning trips. Through user research, 
								it was discovered that 70% of users express a desire for more information about their destination before traveling. 
								Additionally, users tend to question the credibility of information if they are unfamiliar with the brand providing it.

								To meet these user needs and establish credibility, the team aims to develop an application that recommends ideal holiday destinations
								based on real-time data regarding weather conditions and available hotels in the area. The application will utilize the following variables:
								Weather information and Hotel availability.
								
								<br>
                                The primary goals of this project involve collecting the necessary data and building the infrastructure to support the application. 
								Since the project is in its initial stages, there is no existing data available. As a result, your responsibilities will include:

								Scraping data from various destinations, Gathering weather data for each destination, Obtaining information about hotels in each destination,
								Storing all collected information in a data lake and Extracting, transforming, and loading the cleaned data from the data lake to a data warehouse

                                <br>
                                <br>
                                ‚óè <b>Methodology :</b><br>
                                To achieve this project, marketing team wants to focus first on the best cities to travel to in France that can be found from this web site <b><a href="https://one-week-in.com/35-cities-to-visit-in-france/" target="_blank">One Week In.com</a></b>. 
                                <br>
								Weather data will be get from APIs:
								<br> 
								
								<li>  Use  <b><a href="https://nominatim.org/" target="_blank">nominatim</a> </b> to get the gps coordinates of all the cities (no subscription required) </li>

								<li>  Use <b><a href="https://openweathermap.org/appid" target="_blank">openweathermap</a></b>  (you have to subscribe to get a free apikey) </li>
								
								<br>
                                <br>
                                
                                ‚óè <b>Skills Developed:</b><br>
                                Through this project, I developed several skills, including :
                                <br>
                                üõ† <b>Data Scraping and Web Crawling : </b> By scraping data from various destinations, I acquireed expertise in web scraping and web crawling techniques. 
								I also learnt how to extract relevant information from websites and automate the process of gathering data.
                                <br>
                                üõ† <b>Data Integration</b>: Working with different sources of data, such as weather data and hotel information, require integrating data from multiple systems. 
								I developped skills in data integration, ensuring that disparate data sources are combined effectively.
                                <br>
                                üõ† <b>Data Storage and Management</b> : Storing collected data in a data lake and managing it efficiently enhanced my skills in data storage and management. 
								I understood how to structure and organize data to ensure easy retrieval and analysis.
                                <br>
                                üõ† <b>ETL (Extract, Transform, Load) Processes</b> : Extracting, transforming, and loading data from a data lake to a data warehouse is a crucial step in data processing. 
								By performing these tasks, I gained expertise in ETL processes, which are fundamental to data integration and data warehousing.


                                <br>
                                <br>
                                ‚óè <b>Context :</b>
                                This project is just one example of my experience  during my <b>JEDHA BOOTCAMP</b> training.


                                <br>
                                <br>


                            </p>



							



							<p>
								<h3 id="project3">Project 3 : Walmart: Predict Weekly Sales </h3>

                                ‚óè <b>Github repository link    :</b>    <a href="https://github.com/ChrisDMI/Walmart-Sales-Prediction.git">   Github Walmart, Predict Weekly Sales </a> 
                                <br>
                                <br>

                                ‚óè <b>Overview :</b><br>
                                In collaboration with Walmart's marketing service, this project aims to develop a machine learning model capable of accurately estimating the weekly sales in their stores. 
								The model's primary objective is to provide precise predictions that will enable a better understanding of how sales are influenced by economic indicators. 
								Additionally, the predictions generated by the model can assist in planning future marketing campaigns.


                                <br>
                                <br>
                                ‚óè <b>Methodology :</b><br>
                                The project can be divided into three main parts:
                                <br>
													
								<li>  <b>Part 1: Exploratory Data Analysis and Preprocessing</b> </li>
								This initial phase involves conducting exploratory data analysis (EDA) and performing necessary preprocessing steps to prepare the data for machine learning. 
								EDA will help uncover insights and patterns within the dataset, while preprocessing techniques will ensure the data is in a suitable format for training the model.
								<br>

								<li>  <b>Part 2: Baseline Linear Regression Model</b>   </li>
								The next step focuses on training a baseline linear regression model. This model will serve as the initial predictor for estimating weekly sales. 
								By leveraging the principles of linear regression, it will establish a foundation for predicting sales based on the available data and economic indicators.
								<br>

								<li>  <b>Part 3: Regularized Regression Model to Avoid Overfitting</b>   </li>
								To mitigate overfitting and improve the model's generalization capabilities, the third part of the project entails training a regularized regression model. 
								Regularization techniques, such as L1 (Lasso) or L2 (Ridge) regularization, will be applied to penalize excessive model complexity and enhance performance on unseen data.
								<br>
                                <br>
                                
                                ‚óè <b>Skills Developed:</b><br>
                                After completing the Walmart: Predict Weekly Sales project, I developed several valuable skills. Some of these skills include:
                                <br>
                                üõ† <b>Exploratory Data Analysis (EDA): </b> Through conducting EDA as part of the project, I gained experience in analyzing and visualizing data to uncover patterns, trends, and relationships. 
								This skill is crucial for understanding the dataset and making informed decisions about data preprocessing and modeling.
                                <br>
                                üõ† <b>Data Preprocessing: </b> The project involved performing various data preprocessing techniques, such as handling missing values, scaling features, and encoding categorical variables. 
								I developed skills in preparing data for machine learning models, ensuring data quality and compatibility.
                                <br>
                                üõ† <b>Machine Learning Modeling:</b>  By training linear regression models as part of the project, I honed my skills in implementing and evaluating machine learning algorithms. 
								I gained practical knowledge of model fitting, interpretation, and making predictions based on the learned patterns in the data.
                                <br>
                                üõ† <b>Model Evaluation and Selection:</b>  Throughout the project, I learned how to assess the performance of machine learning models using appropriate evaluation metrics. 
								I acquired the ability to compare different models and select the most suitable one based on their predictive accuracy and generalization capabilities.
								<br>
                                üõ† <b>Regularization Techniques:</b>  As part of addressing overfitting, I gained expertise in applying regularization techniques, such as L1 (Lasso) or L2 (Ridge) regularization. 
								These techniques help control model complexity and improve generalization on unseen data.
								<br>
                                üõ† <b>Feature Engineering:</b>  To improve the predictive power of the model, feature engineering techniques might have been employed. This involves transforming and creating new features from the available data. 
								I developed skills in identifying relevant features and engineering them appropriately.
								<br>
                                üõ† <b>Problem Solving and Decision Making:</b>  Throughout the project, I faced various challenges, such as handling data inconsistencies, selecting appropriate modeling approaches, and optimizing model performance. 
								By overcoming these challenges, I enhanced my problem-solving and decision-making abilities in a data-driven context.
								

                                <br>
                                <br>
                                ‚óè <b>Context :</b>
                                This project is just one example of my experience  during my <b>JEDHA BOOTCAMP</b> training.


                                <br>
                                <br>


                            </p>









							<p>
								<h3 id="project4">Project 4 : Conversion rate challenge </h3>

                                ‚óè <b>Github repository link    :</b>    <a href="https://github.com/ChrisDMI/conversion-rate.git">   Github Conversion rate challenge </a> 
                                <br>
                                <br>

                                ‚óè <b>Overview :</b><br>
                                In this project, the objective was to analyze the behavior of users visiting a newsletter website and develop a predictive model to determine if a user is likely to subscribe to the newsletter. 
								The data scientists responsible for the newsletter sought to gain insights into user behavior and identify key factors that influence newsletter subscription rates. 
								They aimed to leverage this information to improve the newsletter's conversion rate. 
								To facilitate this analysis, a competition was organized, inviting teams to build a model that accurately predicts conversions, i.e., when a user subscribes to the newsletter. 
								The organizers provided an open-source dataset containing relevant information about website traffic.

								
                                <br>
                                <br>
                                ‚óè <b>Methodology :</b><br>
                                The model's performance was evaluated based on the f1-score, a metric that balances precision and recall. Teams were ranked according to their model's performance in predicting conversions.
								<br>
                                <br>
                                
                                ‚óè <b>Skills Developed:</b><br>
                                After completing this project, I developed several valuable skills. Some of these skills include:
                                <br>
                                üõ† <b>Data Analysis: </b> Through analyzing the provided dataset, I gained expertise in exploring and understanding complex data, identifying patterns, and extracting meaningful insights. 
								This skill is essential for making informed decisions and driving improvements based on data-driven observations.
                                <br>
                                üõ† <b>Predictive Modeling:  </b> Building a predictive model to determine newsletter subscription behavior allowed me to develop skills in applying machine learning algorithms. 
								I gained hands-on experience in training and evaluating models, selecting appropriate features, and optimizing model performance.
                                <br>
                                üõ† <b>Feature Selection:</b>  The project required identifying and selecting the most relevant features that influence newsletter subscriptions. 
								I acquired skills in feature selection techniques, understanding the importance of different variables, and identifying key factors driving user behavior.
                                <br>
                                üõ† <b>Model Evaluation: </b>  Evaluating the performance of the predictive model using the f1-score provided me with valuable experience in model assessment. 
								I learned how to interpret and analyze evaluation metrics, compare model performance against other teams, and make informed judgments about model effectiveness.
								<br>
                                üõ† <b>Actionable Insights:</b>  By exploring the model's parameters and feature importance, I gained the ability to derive actionable insights. 
								I learned to identify patterns and behaviors that impact newsletter conversions, providing valuable recommendations to improve the conversion rate.
								<br>
                                üõ† <b>Business Understanding: </b>  The project's objective centered around improving the newsletter's conversion rate, requiring me to align technical solutions with business goals. 
								I developed skills in understanding business requirements, translating them into analytical tasks, and delivering actionable recommendations.
								<br>

                                <br>
                                <br>
                                ‚óè <b>Context :</b>
                                This project is just one example of my experience  during my <b>JEDHA BOOTCAMP</b> training.


                                <br>
                                <br>


                            </p>








							<p>
								<h3 id="project5">Project 5 : The North Face e-commerce : boosting online sales ! </h3>

                                ‚óè <b>Github repository link    :</b>    <a href="https://github.com/ChrisDMI/The-North-Face-e-commerce.git">   Github The North Face e-commerce </a> 
                                <br>
                                <br>

                                ‚óè <b>Overview :</b><br>
                                In collaboration with the marketing department, this project aimed to leverage machine learning solutions to enhance online sales on  <a href="https://www.thenorthface.fr/">   The North Face website </a> . 
								Two major solutions were identified to significantly impact conversion rates and improve the overall user experience.

								The project focused on the following objectives:
								<br>
								<b>Recommender System Deployment</b>
								<br>
								One key solution involved implementing a recommender system to suggest additional products to users based on their interests. 
								The goal was to personalize the user experience by displaying a "You might also be interested in these products..." section on each product page. 
								By leveraging machine learning algorithms, the recommender system aimed to provide tailored product recommendations, ultimately increasing conversion rates.

								<br>
								<b>Topic Extraction for Product Catalog Improvement</b>
								<br>
								Another crucial solution centered around improving the structure of the product catalog through topic extraction. The objective was to utilize unsupervised learning methods to challenge the existing categories within the catalog. 
								The project aimed to identify new categories or themes within the product descriptions, making the navigation and exploration of products on the website more suitable and user-friendly.
								
                                <br>
                                <br>
                                ‚óè <b>Methodology :</b><br>
                                The project involved the following three steps:
								
								<br>
								<b>Grouping Similar Products based on Descriptions :</b>
								<br>
								The initial step focused on identifying groups of products with similar descriptions. By employing natural language processing (NLP) techniques, the project aimed to cluster products based on shared characteristics and descriptions. 
								This step laid the foundation for building a recommender system.

								<br>
								<b>Building a Recommender System Algorithm : </b>
								<br>
								Using the groups of similar products, the project aimed to develop a simple yet effective recommender system algorithm. This algorithm would utilize user preferences, historical data, and product similarity to generate personalized recommendations for each user. 
								The goal was to enhance the user experience by providing relevant suggestions, ultimately driving conversion rates.

								<br>
								<b>Topic Modeling for Item Description Analysis :</b>
								<br>
								The final step involved applying topic modeling algorithms to automatically identify latent topics within the item descriptions. By utilizing unsupervised learning techniques, the project aimed to uncover underlying themes or categories present in the product descriptions. 
								This analysis would help improve the structure and organization of the product catalog, facilitating easier navigation and exploration for website visitors.
								
								
								
								<br>
                                <br>
                                
                                ‚óè <b>Skills Developed:</b><br>
                                After successfully completing this project, I developed several valuable skills. Some of these skills include:
                                <br>
                                üõ† <b>Recommender Systems: </b> By implementing a recommender system, I gained hands-on experience in building personalized recommendation algorithms. 
								I learned how to leverage user preferences and item similarities to generate tailored product suggestions, enhancing the user experience and increasing conversion rates.
                                <br>
                                üõ† <b>Natural Language Processing (NLP):  </b> The project involved utilizing NLP techniques to group similar products based on their descriptions and analyze item descriptions for topic extraction. 
								I developed skills in text preprocessing, clustering, and topic modeling, enabling me to handle and extract insights from textual data.
                                <br>
                                üõ† <b>Unsupervised Learning: </b>  Applying unsupervised learning techniques for grouping similar products and topic extraction allowed me to develop skills in unsupervised learning algorithms. 
								I learned how to leverage these techniques to identify patterns, clusters, and latent topics within the data without labeled training data.
                                <br>
                                üõ† <b>Data Analysis and Exploration:  </b>  The project required analyzing and exploring product data to identify groups of similar products and extract latent topics. 
								I gained experience in data exploration, identifying patterns, and applying statistical techniques to derive meaningful insights.
								<br>
                                üõ† <b>Machine Learning Algorithms: </b>  Building and deploying machine learning algorithms for recommender systems and topic modeling enhanced my understanding and proficiency in various machine learning algorithms. 
								I gained experience in algorithm selection, model training, and evaluation.
								<br>
                                üõ† <b>Data Preprocessing:  </b>  The project involved preprocessing product descriptions and textual data, including text cleaning, tokenization, and feature engineering. 
								I acquired skills in data preprocessing techniques specific to textual data, preparing it for further analysis and modeling.
								<br>
                                üõ† <b>Business Understanding:  </b>  The project revolved around boosting online sales and enhancing the user experience. 
								I gained a deeper understanding of aligning machine learning solutions with business objectives, translating them into actionable strategies, and driving measurable outcomes.
								
                                <br>
                                <br>
                                ‚óè <b>Context :</b>
                                This project is just one example of my experience  during my <b>JEDHA BOOTCAMP</b> training.


                                <br>
                                <br>


                            </p>









							<p>
								<h3 id="project6">Project 6 : Hot-Zone Recommendation for Uber Drivers </h3>

                                ‚óè <b>Github repository link    :</b>    <a href="https://github.com/ChrisDMI/uber-hot-zone.git">   Github Hot-Zone Recommendation for Uber Drivers </a> 
                                <br>
                                <br>

                                ‚óè <b>Overview :</b><br>
                                In this project, the objective was to address the issue of drivers not being available in proximity to users when requested. 
								Uber's data team identified a pain point where users often experience delays in getting a ride due to drivers being located in different areas of the city. 
								To minimize wait times and improve user satisfaction, the team aimed to develop a solution that recommends hot-zones in major cities for drivers to be present at any given time of day.

								The project involved the following goals:
								<br>
								<b>Hot-Zone Algorithm Development</b>
								<br>
								The primary objective was to create an algorithm that identifies hot-zones in major cities. Leveraging Uber's existing data on pickups, the algorithm aimed to analyze historical patterns and real-time demand to determine areas with high ride requests. 
								By identifying these hot-zones, drivers can be directed to strategic locations to ensure shorter wait times for users.
								<br>
								<b>Visualization on a Dashboard</b>
								<br>
								To effectively communicate the hot-zone recommendations, the project aimed to develop a visually appealing and informative dashboard. The dashboard would display the identified hot-zones on a map, allowing both drivers and Uber's operational team to visualize the areas of high demand. 
								This visualization would facilitate decision-making and enable efficient allocation of resources.
                                <br>
                                <br>
                                ‚óè <b>Methodology :</b><br>
                                To achieve the project goals of recommending hot-zones for Uber drivers based on pickup data, I used the following methodology:
								
								<br>
								<b>Hot-Zone Identification :</b>
								<br>
								I analyzed the clustered pickup data and derived features to identify hot-zones in major cities. I was focused initially on a specific day and hour to build a prototype. I will apply the clustering algorithm DBSCAN to this subset of data and evaluate the resulting clusters' coordinates to determine the hot-zones.

								<br>
								<b>Visualization with Plotly: </b>
								<br>
								I utilized the Plotly library to create interactive maps that showcase the identified hot-zones. I plotted the cluster coordinates on the map, highlighting areas with high demand and indicating recommended locations for drivers to be present. 
								This visualization will serve as a visual aid for both drivers and the operational team to understand and act upon the hot-zone recommendations.
								<br>
								<b>Generalization and Scalability:</b>
								<br>
								Once the initial hot-zone recommendation prototype is created, expand the approach to cover different days of the week and various hours. I generalized the algorithm to incorporate multiple time frames and evaluate the performance of the hot-zone recommendations in diverse scenarios. 
								
								
								
								<br>
                                <br>
                                
                                ‚óè <b>Skills Developed:</b><br>
                                After successfully completing this project, I developed several valuable skills. Some of these skills include:
                                <br>
                                üõ† <b>Clustering Techniques: </b> By applying clustering algorithms such as <b>K-means</b> or <b>DBSCAN</b> to group pickup locations, I gained hands-on experience in unsupervised learning and clustering analysis. 
								I learned how to identify patterns in data and use cluster coordinates to pinpoint hot zones.
                                <br>
                                üõ† <b>Spatial Analysis: </b> Analyzing pickup locations and identifying hot zones allowed me to develop skills in spatial analysis. 
								I learned how to extract meaningful insights from spatial data and interpret results in the context of geographic locations.
                                <br>
                                üõ† <b>Data Visualization:  </b>  Utilizing the Plotly library to create interactive maps for visualizing hot-zones improved my data visualization skills. 
								I learned how to effectively communicate insights and recommendations through interactive and visually appealing dashboards.
                                <br>
                                üõ† <b>Problem-solving and Optimization:  </b>  Addressing the challenge of recommending hot-zones for Uber drivers required problem-solving skills. 
								I gained experience in optimizing algorithms, adjusting clustering parameters, and fine-tuning the hot-zone recommendation process to improve accuracy and performance.
								<br>
                                üõ† <b>Time Series Analysis: </b>  Considering temporal factors, such as different hours of the day and days of the week, expanded my knowledge in time series analysis. 
								I learned how to analyze and interpret time-dependent patterns in data and adjust recommendations accordingly.
								
                                <br>
                                <br>
                                ‚óè <b>Context :</b>
                                This project is just one example of my experience  during my <b>JEDHA BOOTCAMP</b> training.


                                <br>
                                <br>


                            </p>















							<p>
								<h3 id="project7">Project 7 : Spam detector </h3>

                                ‚óè <b>Github repository link    :</b>    <a href="https://github.com/ChrisDMI/spam-detector.git">   Spam detector </a> 
                                <br>
                                <br>

                                ‚óè <b>Overview :</b><br>
                                As part of my work experience with AT&T dataset, I undertook a project to address the challenge of spam messages that AT&T users frequently encounter. 
								The goal of the project was to develop an automated spam detection system that could identify and flag spam messages based solely on their content.
								AT&T, being the world's largest telecommunications company by revenue, aimed to enhance user experience and protect their customers by implementing an efficient and accurate spam detection solution.


								
                                <br>
                                <br>
                                ‚óè <b>Methodology :</b><br>
                                To achieve optimal results, I followed certain guiding principles and utilized helpful techniques:
								
								<br>
								<b>Starting with simplicity: </b>
								<br>
								I recognized that a robust deep learning model doesn't always have to be excessively complex. 
								By building a solid foundation, I ensured the model would effectively identify spam messages without unnecessary complexity.
								<br>
								<b>Leveraging transfer learning: </b>
								<br>
								Considering the limited availability of training data, I harnessed the power of transfer learning. 
								By leveraging pre-trained models trained on massive datasets, I tapped into their knowledge and adapted it to the specific task of spam detection.
								
								<br>
								<br>

								Throughout the project, I applied a combination of natural language processing techniques, deep learning methodologies, and model evaluation strategies to optimize the performance of the spam detection system. 
								The ultimate objective was to provide AT&T users with an automated and reliable defense against spam messages, thereby improving their overall messaging experience.
								
								<br>
                                <br>
                                
                                ‚óè <b>Skills Developed:</b><br>
                                After completing the AT&T Inc spam detector project, I had the opportunity to develop several valuable skills, including:
                                <br>
                                üõ† <b>Natural Language Processing (NLP): </b> Through this project, I gained expertise in processing and analyzing text data, 
								including techniques such as tokenization, text cleaning, and feature extraction specifically tailored for NLP tasks.
                                <br>
                                üõ† <b>Deep Learning:  </b> Building the spam detector involved working with deep learning models, including neural networks, Long Short-Term Memory (LSTM) , or Word Embedding layer. 
								I learned how to design, train, and fine-tune these models for text classification tasks.
                                <br>
                                üõ† <b>Transfer Learning:  </b>  Leveraging pre-trained models for transfer learning was one of goals of this project. 
								I acquired the skills to adapt and fine-tune existing models, such as those trained on large-scale text datasets, for specialized tasks like spam detection.
                                <br>
                                üõ† <b>Model Evaluation and Metrics:  </b>  I developed a deep understanding of evaluating model performance for classification tasks. 
								This included selecting appropriate evaluation metrics like accuracy, precision, recall, and F1-score, as well as employing techniques like cross-validation and model interpretation.
								<br>
                                üõ† <b>Data Preprocessing and Feature Engineering: </b>  Working with the spam dataset required me to preprocess and transform the data to ensure it was suitable for training and evaluation. 
								I acquired skills in data cleaning, handling missing values, and feature engineering to enhance model performance.
								
                                <br>
                                <br>
                                ‚óè <b>Context :</b>
                                This project is just one example of my experience  during my <b>JEDHA BOOTCAMP</b> training.


                                <br>
                                <br>


                            </p>


























							<p>
								<h3 id="project8">Project 8 : Getaround, Car Rental Check-in and Checkout Analysis	</h3>

                                ‚óè <b>Github repository link    :</b>    <a href="https://github.com/ChrisDMI/Get-Around.git">   Github Getaround </a> 
                                <br>
                                <br>

                                ‚óè <b>Overview :</b><br>
                                In this project, we conducted an analysis to address the issue of late returns during the checkout process in car rentals.
								The car rental process involved three distinct flows: mobile rental agreement on native apps, Connect (where the driver doesn't meet the owner and opens the car with their smartphone), and paper contracts (which were negligible).

								<br>
								Our goal was to mitigate the friction caused by late returns, which often led to unsatisfied customers and rental cancellations.
								
								<br>
                                <br>
                                ‚óè <b>Methodology :</b><br>
                                To tackle this issue, we proposed implementing a minimum delay between two rentals. This meant that if the requested check-in or checkout times were too close to an already booked rental, the car would not be displayed in the search results. 
								However, implementing this feature could potentially impact Getaround's and owners' revenues, so finding the right trade-off was crucial.
								<br>
								To assist the Product Manager in making informed decisions, we conducted several data analyses to gain insights and facilitate the discussion. Here are the initial analyses performed:
								<br>
								<b>Share of Owner's Revenue Affected: </b>
								<br>
								We assessed the proportion of owner's revenue that would potentially be affected by the minimum delay feature. This analysis helped quantify the potential impact on revenue based on different thresholds and scopes chosen.
								
								<br>
								<b>Late Check-in Frequency and Impact: </b>
								<br>
								We assessed the proportion of owner's revenue that would potentially be affected by the minimum delay feature. This analysis helped quantify the potential impact on revenue based on different thresholds and scopes chosen.
								
								<br>
								<b>Problematic Cases Resolved:  </b>
								<br>
								We determined the number of problematic cases that would be resolved by implementing the minimum delay feature, considering different threshold and scope options. This analysis quantified the potential benefits in terms of avoiding late returns and enhancing customer satisfaction.
								
								<br>
                                <br>
                                
                                ‚óè <b>Skills Developed:</b><br>
                                After completing the Car Rental Check-in and Checkout Analysis project, I have developed the following skills:
                                <br>
                                üõ† <b>Web Dashboard Development: </b> I gained experience in building a web dashboard using technologies like <b>streamlit</b> or other suitable frameworks. 
								This involved creating interactive visualizations and displaying relevant data to assist the Product Management team in making informed decisions.
								<br>
                                üõ† <b>Machine Learning Deployment:  </b> I acquired knowledge and expertise in deploying a machine learning model as an API endpoint. 
								I implemented at least one endpoint, <b>/predict</b>, which accepted POST requests with JSON input data and provided predictions as responses. 
								I learned to handle the API requests, process the input data, and return the predictions in a JSON format.
                                <br>
                                üõ† <b>API Documentation:  </b>  I created a documentation page located at <b>/docs</b> of the website to provide users with information about the API. 
								The documentation included a title, descriptions of the available endpoints (including the endpoint name, HTTP method, required input, and expected output), and examples to guide users on how to interact with the API effectively.
                                <br>
                                üõ† <b>Online Production and Hosting:  </b>  I gained experience in hosting the API online using a hosting provider like Heroku. 
								I set up the necessary infrastructure to make the API accessible online, ensuring its availability for users to make requests and receive responses.
								<br>
                                üõ† <b>Data Analysis and Insights: </b>  I performed thorough data analysis to gain insights into the car rental check-in and checkout process. 
								I explored the data to uncover patterns, trends, and important metrics related to late returns and their impact on subsequent rentals. 
								This analysis helped in identifying issues, quantifying potential impacts, and providing valuable insights for decision-making.
								<br>
								üõ† <b>MLflow Tracking: </b> Logging the trained model in MLflow involves utilizing MLflow's tracking capabilities. I learned how to set up MLflow, 
								create experiments, and log relevant information such as model parameters, metrics, and artifacts in <b>S3 Bucket</b>. This skill enables reproducibility and facilitates model versioning and management.
                                
								<br>

								üõ† <b>Model Versioning and Experiment Management:</b> With MLflow, I gained expertise in managing model versions and tracking experiments. 
								I learned how to create different versions of a model, record model metadata, and compare performance across different iterations. This skill was invaluable for maintaining a comprehensive record of model development and facilitating collaboration with team members.                                
								<br>

								üõ† <b>Artifact Management with S3: </b> As part of the project, I leveraged S3 to store and manage model artifacts. I learned how to upload trained models and associated files to S3 for easy access and sharing. 
								This involved understanding bucket permissions, managing object keys, and securely storing sensitive information.
								<br>

								üõ† <b>Model Deployment:  </b> Through the project, I gained insights into the process of deploying machine learning models for inference. 
								I learned how to package trained models, create APIs for model serving, and ensure scalability and reliability of the deployed system. This skill enabled me to deliver a fully functional and production-ready solution.
								<br>

								üõ† <b>Cloud Infrastructure:  </b> Working with MLflow and S3 exposed me to cloud infrastructure components. I gained an understanding of cloud services and how they can be leveraged for machine learning projects. 
								This included concepts such as cloud storage, permissions, and integrating cloud services into the overall architecture.
								<br>

								üõ† <b>Troubleshooting and Debugging: </b> During the model training and deployment phases, I encountered various challenges and learned how to troubleshoot and debug issues effectively. 
								I developed skills in identifying and resolving common errors, optimizing model performance, and ensuring the reliability of the deployed system.
								<br>

                                üõ† <b>Code Sharing and Version Control: </b>  I created a GitHub repository to share my code and provide documentation for the project. 
								I wrote a <b>README.md</b> file describing the project, including setup instructions for local development and the URL for accessing the online API.
                                <br>
                                <br>
                                ‚óè <b>Context :</b>
                                This project is just one example of my experience  during my <b>JEDHA BOOTCAMP</b> training.


                                <br>
                                <br>


                            </p>







							<p>
								<h3 id="group_project1">Group Project 1 : Netflix Recommendation Engine	</h3>

                                ‚óè <b>Github repository link    :</b>    <a href="https://github.com/ChrisDMI/netflix-recommandation-engine.git">   Netflix Recommendation Engine </a> 
                                <br>
                                <br>

                                ‚óè <b>Overview :</b><br>
                                In this project, our team of Machine Learning Engineers took on the challenge of creating a recommendation engine for Netflix, the leading video streaming platform with over 221.6 million subscribers worldwide. 
								With increasing competition from other streaming platforms, Netflix aimed to enhance user experience and attract new subscribers by leveraging data and building a powerful recommendation engine.
								<br>
								Our primary goals were as follows:
								<br>
								<b>Create a recommendation engine using any AI library known to us.</b>
								<br>
								<b>Develop an infrastructure capable of ingesting real-time user interactions on the platform.</b>
								<br>
								<b>Automatically generate real-time recommendations after a user finishes watching a movie.</b>
								
								<br>
								<br>
								‚óè <b>Data Sources :</b><br>
								To accomplish our project goals, we utilized the following data sources:
								<br>
								<b>Netflix Prize Data: </b>This dataset provided a significant amount of labeled data, including information on payments and fraudulent activities. 
								We studied the work of renowned data scientists who had previously explored this dataset, drawing inspiration from their findings and building upon their algorithms.
								<br>
								<b>Jedha x Netflix Real-time User Interactions API: </b> We leveraged this API to retrieve real-time information about movies watched by users. 
								This data was crucial for creating timely and relevant recommendations.

								<br>
                                <br>
                                ‚óè <b>Methodology :</b><br>
                                Our team employed state-of-the-art AI libraries and techniques to develop the recommendation engine. We built upon existing research and adapted algorithms based on the Netflix Prize Data to ensure accurate predictions. 
								The infrastructure was designed to capture real-time user interactions, allowing us to constantly update and improve the recommendations provided to users.

								<br>
                                <br>
                                ‚óè <b>Impact :</b><br>
                                The successful implementation of the recommendation engine had significant implications for Netflix. By delivering personalized and engaging recommendations, the platform aimed to enhance user satisfaction, increase user retention rates, and attract new subscribers. 
								The engine's ability to provide real-time recommendations ensured that users would always have relevant content options to explore, leading to a more immersive streaming experience.

								<br>
                                <br>
                                ‚óè <b>Conclusion :</b><br>
                                Through this project, we demonstrated our expertise in developing advanced recommendation systems using AI libraries. We successfully built a recommendation engine for Netflix, leveraging data analysis and real-time user interactions. 
								The project showcased our ability to work on complex challenges and deliver practical solutions that drive user engagement and business growth.

								<br>
                                <br>
                                
                                ‚óè <b>Technical Skills Developed:</b><br>
                                During the group project of developing a Netflix Recommendation Engine, we had the opportunity to acquire and enhance various technical skills. 
								Here is a detailed overview of the skills we developed:
                                <br>
                                üõ† <b>Web Dashboard Development: </b> We gained experience in building a web dashboard using technologies like <b>streamlit</b>. 
								This involved creating interactive visualizations and displaying relevant data to assist the Product Management team in making informed decisions.
								<br>
                                üõ† <b>Recommendation Engine Development:   </b> We gained expertise in building recommendation engines, which involved understanding 
								the underlying algorithms and techniques used to generate personalized recommendations for Netflix users.
                                <br>
                                üõ† <b>Cloud Computing:   </b>  We used GCP Kubernetes clusters as cloud computing platform. 
								We gained experience in setting up and managing Kubernetes clusters, configuring nodes, and deploying containerized applications on a cloud infrastructure.
                                <br>
                                üõ† <b>Distributed Computing:  </b>  Training a model with a huge dataset often requires distributed computing frameworks like Ray. 
								We learned how to leverage Ray's capabilities to distribute the workload across multiple nodes or machines, optimizing performance and reducing training time.
								<br>
                                üõ† <b>Model Training with scikit-surprise:  </b>  We developed expertise in using scikit-surprise, a Python library for building recommender systems, specifically the SVD (Singular Value Decomposition) model. 
								We learned to configure and train the SVD model with the large dataset, understanding the parameters and techniques involved in collaborative filtering.
								<br>
                                üõ† <b>MLflow Tracking: </b> Logging the trained model in MLflow involves utilizing MLflow's tracking capabilities. We learned how to set up MLflow, 
								create experiments, and log relevant information such as model parameters, metrics, and artifacts. This skill enables reproducibility and facilitates model versioning and management.
                                
								<br>
                                üõ† <b>Model Serialization and Deserialization: </b> Saving and loading the trained SVD model requires serialization and deserialization techniques. 
								We gained knowledge of how to serialize the model to a file format compatible with MLflow, allowing seamless storage and retrieval of the model for future use or deployment.
								
								<br>
                                üõ† <b>Infrastructure Orchestration: </b> Deploying and managing the entire workflow on Kubernetes with Ray involves infrastructure orchestration skills. 
								We learned to coordinate and monitor the different components of the system, ensuring smooth execution, fault tolerance, and scalability.
								

								<br>
                                üõ† <b>Infrastructure Provisioning: </b> Deploying an API on EC2 requires expertise in provisioning and configuring virtual machines in the cloud.
								We gained skills in setting up EC2 instances, selecting appropriate instance types, configuring security groups, and managing networking settings.

								<br>
                                üõ† <b>Server Configuration and Deployment: </b> Deploying an API on EC2 involves configuring the server environment, installing necessary dependencies, and deploying the API code. 
								We gained experience in managing server configurations, ensuring compatibility with the deployed API, and handling dependencies.
								
								<br>
                                üõ† <b>Real-time Data Ingestion and Processing:  </b> Developing the infrastructure to ingest real-time user interactions on the Netflix platform 
								required us to learn about data streaming and real-time processing frameworks like Apache Kafka or Apache Flink. We acquired skills in efficiently handling and processing large volumes of data in real-time.


								<br>
                                üõ† <b>Version Control System:  </b> Working with complex workflows in a team setting requires effective collaboration and version control practices. 
								We developed skills in coordinating tasks, using Git for version control, resolving conflicts, and maintaining a cohesive and efficient development process.
								

								<br>
                                üõ† <b>Collaboration in a Machine Learning Team:   </b> Working as a team of Machine Learning Engineers, we developed strong collaboration skills, including effective communication, task distribution, version control using Git, and resolving conflicts during code integration. 
								We learned how to work together to achieve common goals and deliver high-quality results.

								<br>
                                <br>
                                ‚óè <b>Context :</b>
                                This project is one of final group project that I made during my <b>JEDHA BOOTCAMP</b> training.


                                <br>
                                <br>


                            </p>





							<p>

								<h3 id="group_project2">Group Project 2 : Low Emission Zone Control 	</h3>


								‚óè <b>Overview :</b><br>
								Our group project aimed to develop an app that could analyze uploaded images or videos of cars, providing information about the car's make and model, as well as detecting any visible license plates. 
								The app also checked if the car was allowed to circulate in a low emission zone determined by the town hall.

								<br>
								<br>
								‚óè <b>Data Source :</b><br>
								The data for our project was obtained from two primary sources. First, we scraped the website <b>Platemania.com</b>, which is a platform dedicated to car enthusiasts and license plates. 
								From this website, we collected a total of <b>176,000</b> car images, representing <b>70</b> different car makes and <b>592</b> car models. These images were used to train our <b>Inceptionv3</b> model for car model recognition.
								Additionally, we utilized a dataset called <b>"French License Plate"</b> to train our <b>YOLOv8</b> model, which is responsible for detecting cars and license plates. This dataset provided the necessary images and annotations specifically focused on French license plates.
								By combining these two data sources, we obtained a comprehensive and diverse collection of car images and license plate data, enabling us to train our models effectively and accurately.

								<br>
								<br>
								‚óè <b>Methodology :</b><br>
								To achieve this, we created a <b>Streamlit</b> dashboard for users to upload car images or videos, which were then stored in an <b>S3 bucket</b>. 
								The dashboard generated an S3 URL, which was sent along with a crit'air threshold to our API developed with <b>FastAPI</b> and deployed on an <b>EC2 instance</b> within a <b>Docker</b> container. 
								The API contained our trained models, including <b>YOLOv8</b> for car and license plate detection, <b>InceptionV3</b> for car model recognition, and LPRnet for reading license plate numbers.
								<br>
								<br>
								The API processed the image or video by sending it to <b>YOLOv8</b>, which identified the positions of cars and license plates. 
								The car crops were then passed to InceptionV3 for car model recognition, while the plate crops were used to extract the license plate numbers. 
								Additionally, the API made a request to another API called <b>"Carte Grise Minute"</b> to obtain real information about the detected license plate number and compare it with the predicted car model and crit'air number.
								<br>
								<br>
								If the predicted car model differed from the real car model, <b>YOLOv8</b> framed the car in blue, indicating an incorrect prediction by <b>InceptionV3</b> or a failure to detect the correct license plate number by <b>LPRnet</b>. 
								If the predicted and real car models matched, <b>YOLOv8</b> compared the crit'air number of the car with the threshold set by the town hall. 
								If the car's crit'air number exceeded the threshold, <b>YOLOv8</b> framed the car in red, indicating that it was not allowed in the low emission zone. Otherwise, the car was framed in green, signifying compliance with the restrictions.
								<br>

								The API stored the processed image or video in <b>S3 bucket</b> and returned the corresponding S3 URL to the <b>Streamlit</b> app, which then retrieved and displayed the processed image or video to the user.


								<br>
								<br>
								‚óè <b>Impact :</b><br>
                                The low emission zone control project can have a positive impact on climate change mitigation efforts. By accurately identifying and flagging cars that do not meet the emission standards set by the town hall, the project helps enforce regulations aimed at reducing air pollution and greenhouse gas emissions.
								By implementing strict criteria for vehicles allowed to circulate in low emission zones, the project encourages the use of cleaner and more environmentally friendly vehicles. This, in turn, can lead to a reduction in harmful emissions such as carbon dioxide, nitrogen oxides, and particulate matter, which contribute to air pollution and climate change. 
								The project's ability to detect non-compliant vehicles and enforce emission standards helps create cleaner and healthier urban environments by promoting the adoption of low-emission vehicles and discouraging the use of high-polluting vehicles in restricted areas. This aligns with global efforts to combat climate change and improve air quality, contributing to the overall goal of transitioning to a more sustainable and low-carbon future.
								<br>
                                <br>

								‚óè <b>Conclusion :</b><br>
								In conclusion, the low emission zone control project has demonstrated the potential to effectively monitor and enforce emission standards in restricted areas. By combining advanced technologies such as computer vision and machine learning, the project successfully detects cars that do not meet the required emission criteria.

								The implementation of the project has several key benefits. Firstly, it helps improve air quality by identifying and restricting high-polluting vehicles from entering low emission zones. This contributes to reducing harmful emissions and creating healthier environments for residents and pedestrians.

								Secondly, the project promotes the use of cleaner and more environmentally friendly vehicles by incentivizing compliance with emission standards. By enforcing restrictions and penalties for non-compliant vehicles, it encourages the adoption of low-emission technologies and supports the transition to a greener transportation system.

								Furthermore, the project enhances the efficiency of monitoring and enforcement processes by automating the detection and identification of vehicles. This reduces the need for manual inspections and increases the accuracy and speed of identifying non-compliant vehicles, leading to more effective control and enforcement measures.

								Overall, the low emission zone control project represents a valuable initiative in tackling air pollution, mitigating climate change, and promoting sustainable transportation practices. Its successful implementation highlights the potential of advanced technologies in supporting environmental initiatives and creating cleaner and more sustainable urban environments.

								<br>
								<br>





								‚óè <b>Technical Skills Developed:</b><br>
                                During the group project of developing a Netflix Recommendation Engine, we had the opportunity to acquire and enhance various technical skills. 
								Here is a detailed overview of the skills we developed:
                                <br>
                                üõ† <b>Web Dashboard Development: </b> We gained experience in building a web dashboard using technologies like <b>streamlit</b>. 
								This involved creating interactive visualizations and displaying relevant data to assist the Product Management team in making informed decisions.
								<br>
                                üõ† <b>Data Scraping:   </b> The ability to scrape data from websites, demonstrated by collecting car images and license plate data from Platemania.com. 
								This skill involves web scraping techniques using libraries like <b>BeautifulSoup</b> or <b>Scrapy</b>.
                                <br>
                                üõ† <b>Image Processing:   </b>  Image processing techniques were utilized to preprocess and enhance the car images before feeding them into the models. 
								This includes tasks such as resizing, cropping, and adjusting image quality to optimize model performance.
                                <br>
                                üõ† <b>Computer Vision:  </b>  Computer vision techniques were employed to detect and identify cars and license plates in the images. Models like <b>YOLOv8</b> were trained to perform object detection, 
								while <b>InceptionV3</b> was used for car's make and model detection and <b>LPRnet</b> for license plate recognition. 
								This involved implementing and fine-tuning pre-trained models using frameworks like <b>TensorFlow</b> or <b>PyTorch</b>.
								<br>
                                üõ† <b> Deep Learning:  </b> Deep learning algorithms and methodologies were utilized to train 3 models. 
								This involved data preprocessing, model training, and evaluation using techniques like transfer learning. Skills in data preprocessing, model selection, and evaluation were developed.
								<br>
                                üõ† <b>API Development:  </b> An API was developed using <b>FastAPI</b> to serve as the interface for making predictions and processing requests. 
								This involved creating API endpoints, handling HTTP requests, and integrating the trained models into the API to perform predictions.
								<br>
                                üõ† <b>Cloud Deployment: </b> The trained models and the API were deployed on cloud platforms like <b>EC2 instance</b> and <b>Docker</b> containers. 
								Skills in deploying applications on cloud infrastructure, managing virtual machines, and containerization were developed.
								
								<br>
                                üõ† <b>Data Storage and Retrieval:  </b> The project involved storing and retrieving data from cloud storage platforms such as <b>S3</b>. 
								Skills in working with cloud storage services and efficiently retrieving data were acquired.
								

								<br>
                                üõ† <b>Collaboration and Version Control: </b> The project likely involved collaboration among team members using version control systems like Git. 
								Skills in managing code repositories, branching, merging, and resolving conflicts were developed.

								
								<br>
                                <br>
                                ‚óè <b>Context :</b>
                                This project is one of final group project that I made during my <b>JEDHA BOOTCAMP</b> training.


                                <br>
                                <br>



								‚óè <b>Video of project :</b><br>

								<video controls="controls" width="600" height="400" name="ZFE-control">
									<source src="https://netflix-project-bucket.s3.eu-west-3.amazonaws.com/data/ZFE-control.mov" >
									Your browser does not support the video tag.
								  </video>
							</p>









						</article>

						<!-- Here is a the "Contact" card. -->
						<article id="contact">
							<h2 class="major">Contact</h2>
							<p> 
								<b>üìß Email :</b> nguiepemarius@gmail.com
							</p>

							<p>
								<b>üìû Mobile :</b> +33 6 66 48 47 74	
							</p>

							<p> 
								<b>üè° Address :</b> Based in Paris, France
							</p>

							<!-- 	Here are the fancy icons that link to your social media.
								These icons are rendered using Font Awesome fonts, and you can find more icons
								here: https://fontawesome.com/icons
							-->
							<ul class="icons">
								<!-- The Linkedin icon -->
								<li>
									<!-- 	Note that you need to change the link to point to your page.
										Replace "#" with your Twitter profile page.
									-->
									<a href="https://www.linkedin.com/in/christian-segnou-4461b2102/" class="icon fa-linkedin">
										<span class="label">Linkedin</span>
									</a>

								<!-- The GitHub icon -->
								<li>
									<a href="https://github.com/ChrisDMI" class="icon fa-github">
										<span class="label">GitHub</span>
									</a>
								</li>
								<!-- 	We can add a new icon by searching on the Font Awesome website
									for the correct class name and inserting it as a new list item.

									We found "envelope" here: https://fontawesome.com/icons/envelope
								-->
								<li>
									<a href="mailto: nguiepemariusd@gmail.com" class="icon fa-envelope">
										<span class="label">Email</span>
									</a>
								</li>
							</ul>
						</article>
					</div>

					<!-- This is the footer that appears at the bottom of the page -->
					<footer id="footer">
						<!-- 	You can change "Untitled" to your name to show everyone that
							this is your work.
						-->
						<p class="copyright">&copy; Untitled. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>

			</div>

			<!--	This is the background image of the site.
				All configuration of the background is done with CSS.
				Look in the file assets/css/main.css and search for "#bg" to
				see how this element is styled. Look for comments pointing 
				to where you can set a new background image.
			-->
			<div id="bg"></div>

			<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>